{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b8325b",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b723581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c6be",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset\n",
    "For this example, I'll use the MNIST dataset, which is a standard dataset for multiclass classification with 10 classes (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d6ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 164s 14us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd57f4",
   "metadata": {},
   "source": [
    "### Define a Function to Build the Model\n",
    "Here’s a simple neural network model that we'll use for both binary and multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fe2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32a5e4",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Model with Different Loss Functions\n",
    "We’ll train the model twice: once with binary cross-entropy (assuming binary classification by reducing the number of classes) and once with categorical cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86941fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Accuracy with Binary Cross-Entropy: 100.00%\n",
      "Multiclass Classification Accuracy with Categorical Cross-Entropy: 97.86%\n"
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(loss_function, num_classes, is_one_hot_encoded=True):\n",
    "    model = build_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), verbose=0)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# For Binary Classification (using a subset of the MNIST dataset, e.g., digits 0 and 1)\n",
    "binary_x_train = x_train[y_train[:, 0] == 1]\n",
    "binary_y_train = y_train[y_train[:, 0] == 1][:, :2]\n",
    "binary_x_test = x_test[y_test[:, 0] == 1]\n",
    "binary_y_test = y_test[y_test[:, 0] == 1][:, :2]\n",
    "\n",
    "binary_model = build_model(2)\n",
    "binary_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "binary_model.fit(binary_x_train, binary_y_train, epochs=5, validation_data=(binary_x_test, binary_y_test), verbose=0)\n",
    "binary_accuracy = binary_model.evaluate(binary_x_test, binary_y_test, verbose=0)[1]\n",
    "\n",
    "# For Multiclass Classification (using the full MNIST dataset)\n",
    "categorical_accuracy = train_and_evaluate('categorical_crossentropy', 10, is_one_hot_encoded=True)\n",
    "\n",
    "# Output the accuracies\n",
    "print(f\"Binary Classification Accuracy with Binary Cross-Entropy: {binary_accuracy * 100:.2f}%\")\n",
    "print(f\"Multiclass Classification Accuracy with Categorical Cross-Entropy: {categorical_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44b86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = [\n",
    "    'binary_crossentropy',\n",
    "    'categorical_crossentropy',\n",
    "    'sparse_categorical_crossentropy',\n",
    "    # You can add more loss functions if relevant\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7142ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(loss_function, x_train, y_train, x_test, y_test, num_classes):\n",
    "    model = build_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), verbose=0)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "227da0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification:\n",
      "Accuracy with binary_crossentropy: 99.95%\n",
      "\n",
      "Multiclass Classification:\n",
      "Accuracy with categorical_crossentropy: 97.67%\n",
      "Accuracy with sparse_categorical_crossentropy: 97.70%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding for multiclass classification\n",
    "y_train_one_hot = to_categorical(y_train, 10)\n",
    "y_test_one_hot = to_categorical(y_test, 10)\n",
    "\n",
    "# Filter for binary classification (digits 0 and 1 only)\n",
    "binary_filter_train = (y_train == 0) | (y_train == 1)\n",
    "binary_filter_test = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "binary_x_train = x_train[binary_filter_train]\n",
    "binary_y_train = y_train[binary_filter_train]\n",
    "binary_x_test = x_test[binary_filter_test]\n",
    "binary_y_test = y_test[binary_filter_test]\n",
    "\n",
    "# Convert the binary labels to one-hot encoding\n",
    "binary_y_train_one_hot = to_categorical(binary_y_train, 2)\n",
    "binary_y_test_one_hot = to_categorical(binary_y_test, 2)\n",
    "\n",
    "# List of loss functions to test\n",
    "loss_functions = [\n",
    "    'binary_crossentropy',\n",
    "    'categorical_crossentropy',\n",
    "    'sparse_categorical_crossentropy',\n",
    "]\n",
    "\n",
    "def build_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(loss_function, x_train, y_train, x_test, y_test, num_classes):\n",
    "    model = build_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), verbose=0)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Train and evaluate the binary classification model\n",
    "print(\"Binary Classification:\")\n",
    "for loss_function in loss_functions:\n",
    "    if loss_function == 'binary_crossentropy':\n",
    "        accuracy = train_and_evaluate(loss_function, binary_x_train, binary_y_train_one_hot, binary_x_test, binary_y_test_one_hot, 2)\n",
    "        print(f\"Accuracy with {loss_function}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train and evaluate the multiclass classification model\n",
    "print(\"\\nMulticlass Classification:\")\n",
    "for loss_function in loss_functions:\n",
    "    if loss_function != 'binary_crossentropy':  # Skip binary cross-entropy for multiclass\n",
    "        if loss_function == 'sparse_categorical_crossentropy':\n",
    "            accuracy = train_and_evaluate(loss_function, x_train, y_train, x_test, y_test, 10)\n",
    "        else:\n",
    "            accuracy = train_and_evaluate(loss_function, x_train, y_train_one_hot, x_test, y_test_one_hot, 10)\n",
    "        print(f\"Accuracy with {loss_function}: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca1701bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with binary_crossentropy: 99.95%\n"
     ]
    }
   ],
   "source": [
    "for loss_function in loss_functions:\n",
    "    # Use binary cross-entropy only for binary classification\n",
    "    if loss_function == 'binary_crossentropy':\n",
    "        binary_y_train_one_hot = to_categorical(binary_y_train, 2)\n",
    "        binary_y_test_one_hot = to_categorical(binary_y_test, 2)\n",
    "        accuracy = train_and_evaluate(loss_function, binary_x_train, binary_y_train_one_hot, binary_x_test, binary_y_test_one_hot, 2)\n",
    "        print(f\"Accuracy with {loss_function}: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ea3df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiclass Classification:\n",
      "Accuracy with categorical_crossentropy: 97.66%\n",
      "Accuracy with sparse_categorical_crossentropy: 97.60%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMulticlass Classification:\")\n",
    "for loss_function in loss_functions:\n",
    "    if loss_function != 'binary_crossentropy':  # Skip binary cross-entropy for multiclass\n",
    "        if loss_function == 'sparse_categorical_crossentropy':\n",
    "            accuracy = train_and_evaluate(loss_function, x_train, y_train, x_test, y_test, 10)\n",
    "        else:\n",
    "            accuracy = train_and_evaluate(loss_function, x_train, y_train_one_hot, x_test, y_test_one_hot, 10)\n",
    "        print(f\"Accuracy with {loss_function}: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068b796",
   "metadata": {},
   "source": [
    "## Using my own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05729d",
   "metadata": {},
   "source": [
    "## Load and Preprocess Your Image Dataset\n",
    "Assume your images are in a single directory and you need to split them into training and testing sets.\n",
    "\n",
    "#### 1.1 Use ImageDataGenerator for Splitting\n",
    "You can use ImageDataGenerator with the validation_split parameter to split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36d8a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 774 images belonging to 3 classes.\n",
      "Found 191 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the directory containing your images\n",
    "dataset_dir = \"D:\\ML A-Z\\DataSets\\Image Classification\"  # Replace with the path to your dataset\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (128, 128)  # Adjust as needed\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator with validation split\n",
    "datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)  # 20% for validation\n",
    "\n",
    "# Load the training data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'binary' if binary classification\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load the validation (test) data\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'binary' if binary classification\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5bf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c7e4be",
   "metadata": {},
   "source": [
    "## Define and Train the Model\n",
    "Now that you have split the dataset, you can proceed with defining, training, and evaluating the model using different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517f2de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 31s 1s/step - loss: 0.8621 - accuracy: 0.7235 - val_loss: 0.2104 - val_accuracy: 0.9058\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 18s 734ms/step - loss: 0.1283 - accuracy: 0.9587 - val_loss: 0.1550 - val_accuracy: 0.9476\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 19s 776ms/step - loss: 0.0752 - accuracy: 0.9755 - val_loss: 0.2323 - val_accuracy: 0.9372\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 23s 924ms/step - loss: 0.0780 - accuracy: 0.9767 - val_loss: 0.1295 - val_accuracy: 0.9476\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 22s 896ms/step - loss: 0.0526 - accuracy: 0.9832 - val_loss: 0.1724 - val_accuracy: 0.9791\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 0.1724 - accuracy: 0.9791\n",
      "Accuracy with Categorical Cross-Entropy: 97.91%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_image_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(loss_function, num_classes):\n",
    "    model = build_image_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(train_generator, epochs=5, validation_data=validation_generator, verbose=1)\n",
    "    loss, accuracy = model.evaluate(validation_generator, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming it's a multiclass problem (e.g., 3 classes)\n",
    "num_classes = 3  # Adjust based on your dataset\n",
    "\n",
    "# Train with categorical cross-entropy\n",
    "categorical_accuracy = train_and_evaluate_model('categorical_crossentropy', num_classes)\n",
    "\n",
    "print(f\"Accuracy with Categorical Cross-Entropy: {categorical_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12362d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2083cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 558 images belonging to 2 classes.\n",
      "Found 138 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the directory containing your images\n",
    "dataset_dir = \"D:\\ML A-Z\\DataSets\\Binary Classification\"  # Replace with the path to your dataset\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (128, 128)  # Adjust as needed\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator with validation split\n",
    "datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)  # 20% for validation\n",
    "\n",
    "# Load the training data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # Use 'binary' if binary classification\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load the validation (test) data\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # Use 'binary' if binary classification\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84246fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 17s 889ms/step - loss: 0.5246 - accuracy: 0.8082 - val_loss: 0.2167 - val_accuracy: 0.8986\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 16s 886ms/step - loss: 0.1506 - accuracy: 0.9409 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 16s 877ms/step - loss: 0.0448 - accuracy: 0.9875 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 16s 887ms/step - loss: 0.0277 - accuracy: 0.9928 - val_loss: 0.0219 - val_accuracy: 0.9928\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 15s 843ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9855\n",
      "5/5 [==============================] - 2s 280ms/step - loss: 0.0293 - accuracy: 0.9855\n",
      "Accuracy with Categorical Binary crossentropy: 98.55%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_image_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(loss_function, num_classes):\n",
    "    model = build_image_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(train_generator, epochs=5, validation_data=validation_generator, verbose=1)\n",
    "    loss, accuracy = model.evaluate(validation_generator, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming it's a multiclass problem (e.g., 3 classes)\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "# Train with categorical cross-entropy\n",
    "categorical_accuracy = train_and_evaluate_model('binary_crossentropy', num_classes)\n",
    "\n",
    "print(f\"Accuracy with Categorical Binary crossentropy: {categorical_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e011f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification:\n",
      "Accuracy with binary_crossentropy: 99.95%\n",
      "\n",
      "Multiclass Classification:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 179. MiB for an array with shape (60000, 28, 28) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m y_train_input \u001b[38;5;241m=\u001b[39m y_train_one_hot \u001b[38;5;28;01mif\u001b[39;00m loss_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y_train\n\u001b[0;32m     49\u001b[0m y_test_input \u001b[38;5;241m=\u001b[39m y_test_one_hot \u001b[38;5;28;01mif\u001b[39;00m loss_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y_test\n\u001b[1;32m---> 50\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(loss_function, x_train, y_train_input, x_test, y_test_input, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_function\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(loss_function, x_train, y_train, x_test, y_test, num_classes)\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     32\u001b[0m     Flatten(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)),\n\u001b[0;32m     33\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     34\u001b[0m     Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m ])\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mloss_function, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 37\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 179. MiB for an array with shape (60000, 28, 28) and data type float32"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Filter for binary classification (digits 0 and 1 only)\n",
    "binary_filter_train = (y_train < 2)\n",
    "binary_filter_test = (y_test < 2)\n",
    "\n",
    "binary_x_train, binary_y_train = x_train[binary_filter_train], y_train[binary_filter_train]\n",
    "binary_x_test, binary_y_test = x_test[binary_filter_test], y_test[binary_filter_test]\n",
    "\n",
    "# Convert binary labels to one-hot encoding\n",
    "binary_y_train_one_hot = to_categorical(binary_y_train, 2)\n",
    "binary_y_test_one_hot = to_categorical(binary_y_test, 2)\n",
    "\n",
    "# Convert multiclass labels to one-hot encoding for categorical_crossentropy\n",
    "y_train_one_hot = to_categorical(y_train, 10)\n",
    "y_test_one_hot = to_categorical(y_test, 10)\n",
    "\n",
    "# List of loss functions to test\n",
    "loss_functions = ['binary_crossentropy', 'categorical_crossentropy', 'sparse_categorical_crossentropy']\n",
    "\n",
    "# Function to build and evaluate the model\n",
    "def evaluate_model(loss_function, x_train, y_train, x_test, y_test, num_classes):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), verbose=0)\n",
    "    return model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "# Evaluate binary classification\n",
    "print(\"Binary Classification:\")\n",
    "binary_accuracy = evaluate_model('binary_crossentropy', binary_x_train, binary_y_train_one_hot, binary_x_test, binary_y_test_one_hot, 2)\n",
    "print(f\"Accuracy with binary_crossentropy: {binary_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate multiclass classification\n",
    "print(\"\\nMulticlass Classification:\")\n",
    "for loss_function in ['categorical_crossentropy', 'sparse_categorical_crossentropy']:\n",
    "    y_train_input = y_train_one_hot if loss_function == 'categorical_crossentropy' else y_train\n",
    "    y_test_input = y_test_one_hot if loss_function == 'categorical_crossentropy' else y_test\n",
    "    accuracy = evaluate_model(loss_function, x_train, y_train_input, x_test, y_test_input, 10)\n",
    "    print(f\"Accuracy with {loss_function}: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208d98a",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### Dataset Preprocessing:\n",
    "\n",
    "##### Binary Classification:\n",
    "We filter out the digits 0 and 1 from the dataset for binary classification.\n",
    "Multiclass Classification: We use the full dataset for multiclass classification.\n",
    "Normalization: The images are normalized to a range of 0-1.\n",
    "#### Loss Functions:\n",
    "\n",
    "We evaluate the model using binary_crossentropy for binary classification and both categorical_crossentropy and sparse_categorical_crossentropy for multiclass classification.\n",
    "#### Model Building and Evaluation:\n",
    "\n",
    "The function evaluate_model builds a simple model, trains it, and returns the accuracy.\n",
    "###### Output:\n",
    "\n",
    "The accuracies for each loss function are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf917f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
